############ works

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime
from pyspark.sql import Window, functions as F

minio_access_key = "g3sB5A0PkrpDJ1iuzxhw"
minio_secret_key = "FcodVGJaEhZdX88zyB9safLSdIMc1vNcCUwyAqSM"
minio_bucket_name = "test"
minio_endpoint = "http://localhost:9000"  # Replace with your MinIO server endpoint

spark = SparkSession.builder.appName("MinioTest") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
        .config("spark.hadoop.fs.s3a.endpoint", minio_endpoint) \
        .config("spark.hadoop.fs.s3a.access.key", minio_access_key) \
        .config("spark.hadoop.fs.s3a.secret.key", minio_secret_key ) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .getOrCreate()


df = spark.read.parquet('s3a://test/subfolder/userdata1.parquet',header=True)
df.show()
#df.write.format('csv').options(delimiter='|').mode('overwrite').save('s3a://test/subfolder/')

#################

# Import the SparkSession module
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
    .getOrCreate()

# Get the SparkContext from the SparkSession
sc = spark.sparkContext

# Set the MinIO access key, secret key, endpoint, and other configurations
sc._jsc.hadoopConfiguration().set("fs.s3a.access.key", "g3sB5A0PkrpDJ1iuzxhw")
sc._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "FcodVGJaEhZdX88zyB9safLSdIMc1vNcCUwyAqSM")
sc._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "http://localhost:9000")
sc._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")
sc._jsc.hadoopConfiguration().set("fs.s3a.connection.ssl.enabled", "false")
sc._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
sc._jsc.hadoopConfiguration().set("fs.s3a.connection.ssl.enabled", "false")

# Read a JSON file from an MinIO bucket using the access key, secret key, 
# and endpoint configured above
df = spark.read.option("header", "true") \
    .json(f"s3a://test/file.json")

df = spark.read.parquet('s3a://test/subfolder/userdata1.parquet',header=True)

# show data
df.show()
